fa_selector:
    dim: 192
    depth: 5
    heads: 8
    mlp_dim: 1024
    dropout: 0.0
    out_dim: 1
    pool: 'mean'
    model_path: 'la-la'
    
student:
    d_numerical: 119
    categories: [2,2,2,2]
    token_bias: True
    d_token: 192 
    # transformer
    n_layers: 10
    n_heads: 8 
    d_ffn_factor: 1.333
    attention_dropout: 0.2
    ffn_dropout: 0.1
    residual_dropout: 0.0
    prenormalization: False
    initialization: 'kaiming'
    activation: 'reglu'
    # linformer
    kv_compression: 
    kv_compression_sharing: 
    #
    d_out: 1
    model_path: 'la-la'